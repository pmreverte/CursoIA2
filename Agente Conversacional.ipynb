{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script installs the necessary Python packages for the project. The packages include:\n",
    "\n",
    "- llama-index: A package for creating and managing indexes.\n",
    "- llama-index-llms-azure-openai: A package for integrating Llama Index with Azure OpenAI.\n",
    "- python-dotenv: A package for reading key-value pairs from a .env file and setting them as environment variables.\n",
    "- pymupdf==1.23.22: A package for working with PDF files.\n",
    "- transformers: A package for state-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.\n",
    "- tiktoken: A package for tokenizing text.\n",
    "- matplotlib: A package for creating static, animated, and interactive visualizations in Python.\n",
    "\"\"\"\n",
    "%pip install llama-index\n",
    "%pip install llama-index-llms-azure-openai\n",
    "%pip install python-dotenv\n",
    "%pip install pymupdf==1.23.22\n",
    "%pip install transformers tiktoken\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script imports various libraries and modules required for different functionalities such as:\n",
    "\n",
    "- `json`: For handling JSON data.\n",
    "- `os`: For interacting with the operating system.\n",
    "- `requests`: For making HTTP requests.\n",
    "- `fitz`: For working with PDF documents.\n",
    "- `matplotlib.pyplot`: For creating visualizations.\n",
    "- `numpy`: For numerical operations.\n",
    "- `pandas`: For data manipulation and analysis.\n",
    "- `tiktoken`: (Assumed to be a custom or third-party library, not a standard Python library)\n",
    "- `dotenv`: For loading environment variables from a .env file.\n",
    "- `openai.AzureOpenAI`: For interacting with Azure OpenAI services.\n",
    "- `llama_index.core.agent.FunctionCallingAgentWorker`: For agent-related functionalities.\n",
    "- `llama_index.core.agent.AgentRunner`: For running agents.\n",
    "- `llama_index.core.tools.FunctionTool`: For using function tools.\n",
    "- `llama_index.llms.azure_openai.AzureOpenAI`: For Azure OpenAI language model services.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import fitz\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.azure_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga de variables de env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script loads environment variables and assigns them to corresponding variables for use in an Azure OpenAI and Bing Search API integration.\n",
    "Environment Variables:\n",
    "- AZURE_OPENAI_ENDPOINT: The endpoint URL for the Azure OpenAI service.\n",
    "- AZURE_OPENNAI_DEPLOYMENT_NAME: The deployment name for the Azure OpenAI service.\n",
    "- AZURE_OPENAI_API_KEY: The API key for accessing the Azure OpenAI service.\n",
    "- AZURE_OPENAI_ENDPOINT_EMBEDINGS: The endpoint URL for the Azure OpenAI embeddings service.\n",
    "- BING_SEARCH_API_KEY: The API key for accessing the Bing Search API.\n",
    "- AZURE_OPENAI_EMBEDINGS_API_KEY: The API key for accessing the Azure OpenAI embeddings service.\n",
    "- AZURE_OPENAI_EMBEDINGS_DEPLOYMENT_NAME: The deployment name for the Azure OpenAI embeddings service.\n",
    "Note:\n",
    "Ensure that the .env file contains the above environment variables with valid values.\n",
    "\"\"\"\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_deployment_name = os.getenv(\"AZURE_OPENNAI_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint_embeding = os.getenv(\"AZURE_OPENAI_ENDPOINT_EMBEDINGS\")\n",
    "bing_search_api_key = os.getenv(\"BING_SEARCH_API_KEY\")\n",
    "azure_openai_embeding_api_key = os.getenv(\"AZURE_OPENAI_EMBEDINGS_API_KEY\")\n",
    "azure_openai_embeding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDINGS_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea el cliente Azure OpenAI en LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializes two instances of the AzureOpenAI class with different configurations.\n",
    "Attributes:\n",
    "    azure_openai_client (AzureOpenAI): An instance of AzureOpenAI configured for general use with the specified engine, endpoint, API key, and API version.\n",
    "    azure_openai_client_embeding (AzureOpenAI): An instance of AzureOpenAI configured for embedding use with a different engine, endpoint, API key, and the same API version.\n",
    "Parameters:\n",
    "    azure_openai_deployment_name (str): The name of the deployment for the general use engine.\n",
    "    azure_openai_endpoint (str): The endpoint URL for the general use Azure OpenAI service.\n",
    "    azure_openai_api_key (str): The API key for authenticating with the general use Azure OpenAI service.\n",
    "    azure_openai_embeding_deployment_name (str): The name of the deployment for the embedding engine.\n",
    "    azure_openai_endpoint_embeding (str): The endpoint URL for the embedding Azure OpenAI service.\n",
    "    azure_openai_embeding_api_key (str): The API key for authenticating with the embedding Azure OpenAI service.\n",
    "\"\"\"\n",
    "azure_openai_client = AzureOpenAI(\n",
    "                                engine = azure_openai_deployment_name,\n",
    "                                azure_endpoint=azure_openai_endpoint,\n",
    "                                api_key=azure_openai_api_key,\n",
    "                                api_version=\"2024-05-01-preview\")\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_openai_client_embeding = AzureOpenAI(\n",
    "                                azure_endpoint=azure_openai_endpoint_embeding,\n",
    "                                api_key=azure_openai_embeding_api_key,\n",
    "                                api_version=\"2024-05-01-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El usuario inicializa el prompt de sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script defines a simple conversational agent system that stores messages in a list.\n",
    "Functions:\n",
    "    store_message_in_list(message_list, message_role, **kwargs):\n",
    "        Stores a message in the provided list with the given role and additional keyword arguments.\n",
    "    get_user_input(prompt, default_message):\n",
    "        Prompts the user for input and returns the input if provided, otherwise returns a default message.\n",
    "Constants:\n",
    "    SYSTEM_MESSAGE:\n",
    "        A system message obtained from user input or a default message if no input is provided.\n",
    "\"\"\"\n",
    "messages = []\n",
    "\n",
    "def store_message_in_list(message_list, message_role, **kwargs):\n",
    "    message = {\"role\": message_role}\n",
    "    message.update(kwargs)\n",
    "\n",
    "    message_list.append(message)\n",
    "\n",
    "def get_user_input(prompt, default_message):\n",
    "        user_input = input(prompt)\n",
    "        if user_input == \"\":\n",
    "            return default_message\n",
    "        return user_input\n",
    "\n",
    "SYSTEM_MESSAGE = get_user_input(\"Prompt de Sistema: \",\"Eres un asistente que ayuda. La respuesta siempre la devolverás en el idioma en el que te hablen.\") + \" Debes de ser breve y conciso en tus respuestas.\"\n",
    "\n",
    "store_message_in_list(messages, \"system\", content=SYSTEM_MESSAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos tool, busqueda bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Searches for data using the Bing Search API and returns the snippet of the first result.\n",
    "Args:\n",
    "    query (str): The search query string.\n",
    "Returns:\n",
    "    str: The snippet of the first search result if available, otherwise \"No results found.\"\n",
    "Raises:\n",
    "    requests.exceptions.HTTPError: If the HTTP request returned an unsuccessful status code.\n",
    "\"\"\"\n",
    "def search_for_data_in_bing(query):\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": bing_search_api_key}\n",
    "    params = {\"q\": query,\"count\": 1}\n",
    "\n",
    "    response = requests.get(\"https://api.bing.microsoft.com/v7.0/search\", headers=headers, params=params)  \n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    if \"webPages\" in data and \"value\" in data[\"webPages\"] and len(data[\"webPages\"][\"value\"]) > 0:\n",
    "        first_result = data[\"webPages\"][\"value\"][0]\n",
    "        snippet = first_result.get(\"snippet\",\"no snippet\")\n",
    "        return snippet\n",
    "    else:\n",
    "        return \"No results found.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos tool de búsqueda en índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the embeddings for a given text using the specified Azure OpenAI client.\n",
    "Args:\n",
    "    text (str): The input text to calculate embeddings for.\n",
    "    client: The Azure OpenAI client to use for generating embeddings. Defaults to azure_openai_client_embeding.\n",
    "Returns:\n",
    "    list: The embeddings for the input text.\n",
    "\"\"\"\n",
    "pass\n",
    "\"\"\"\n",
    "Calculate the cosine similarity between two vectors.\n",
    "Args:\n",
    "    vec1 (numpy.ndarray): The first vector.\n",
    "    vec2 (numpy.ndarray): The second vector.\n",
    "Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "\"\"\"\n",
    "pass\n",
    "\"\"\"\n",
    "Find the most similar documents to the input text from a given dataset.\n",
    "Args:\n",
    "    input_text (str): The input text to compare against the dataset.\n",
    "    data (list): A list of dictionaries containing 'text' and 'embeddings' keys.\n",
    "    desired_doc_count (int): The number of most similar documents to return. Defaults to 1.\n",
    "Returns:\n",
    "    list: A list of tuples containing the most similar documents and their similarity scores.\n",
    "\"\"\"\n",
    "pass\n",
    "\n",
    "def calculate_embeddings(text, client=azure_openai_client_embeding):\n",
    "    embeddings = client.embeddings.create(input=text, model=azure_openai_embeding_deployment_name)\n",
    "\n",
    "    return embeddings.data[0].embedding\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_most_similar(input_text, data, desired_doc_count=1):\n",
    "    input_text_embeding = calculate_embeddings(input_text)\n",
    "    similarities = []\n",
    "    for entry in data:\n",
    "        similarity = cosine_similarity(input_text_embeding, np.array(entry[\"embeddings\"]))\n",
    "        similarities.append((entry[\"text\"], similarity))\n",
    "\n",
    "    sorted_documents =sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_documents[:desired_doc_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localización de los pdfs usados en RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieves PDF files from a specified source, which can be a local directory, a local file, or a URL.\n",
    "Args:\n",
    "    source (str, optional): The path to a local directory, a local PDF file, or a URL to download a PDF file from. \n",
    "                            If not provided or empty, the current working directory is used.\n",
    "Returns:\n",
    "    list: A list of paths to the PDF files found or downloaded.\n",
    "The function performs the following actions based on the source:\n",
    "- If the source is a URL, it downloads the PDF file from the URL if it does not already exist locally.\n",
    "- If the source is a local directory, it lists all PDF files in the directory.\n",
    "- If the source is a local PDF file, it adds the file to the list.\n",
    "- If the source is invalid, it prints an error message.\n",
    "Example:\n",
    "    pdf_files = get_pdfs(\"http://example.com/sample.pdf\")\n",
    "    pdf_files = get_pdfs(\"/path/to/local/directory\")\n",
    "    pdf_files = get_pdfs(\"/path/to/local/file.pdf\")\n",
    "\"\"\"\n",
    "def get_pdfs(source=None):\n",
    "    pdf_files = []\n",
    "\n",
    "    if source is None or source == \"\":\n",
    "        source = os.getcwd()\n",
    "\n",
    "    if source.startswith(\"http://\") or source.startswith(\"https://\"):\n",
    "        local_filename = source.split(\"/\")[-1]\n",
    "        if not local_filename.endswith(\".pdf\"):\n",
    "            local_filename += \".pdf\"\n",
    "\n",
    "        if not os.path.exists(local_filename):\n",
    "            print(f\"File {local_filename} does not exist. Downloading from {source}...\")\n",
    "\n",
    "            response = requests.get(source)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                with open(local_filename, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"File downloaded and saved as {local_filename}\")\n",
    "                pdf_files.append(local_filename)\n",
    "            else:\n",
    "                print(f\"Failed to download file: {response.status_code}\")\n",
    "        else:\n",
    "            print(f\"File {local_filename} already exists.\")\n",
    "            pdf_files.append(local_filename)\n",
    "    else:\n",
    "        if os.path.isdir(source):\n",
    "            for file in os.listdir(source):\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_files.append(os.path.join(source, file))\n",
    "        elif os.path.isfile(source) and source.endswith(\".pdf\"):\n",
    "            pdf_files.append(source)\n",
    "        else:\n",
    "            print(f\"Invalid source: {source}\")\n",
    "\n",
    "    return pdf_files\n",
    "\n",
    "pdf_files = get_pdfs(input(\"Indica la ruta del archivo o archivos en PDF o la URL de descarga del archivo PDF: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorar los archivos pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Formats the given text by replacing newline characters with spaces and stripping leading/trailing whitespace.\n",
    "Args:\n",
    "    text (str): The text to be formatted.\n",
    "Returns:\n",
    "    str: The formatted text.\n",
    "\"\"\"\n",
    "pass\n",
    "\"\"\"\n",
    "Opens a PDF file, reads its content, and extracts text from each page. The text is then tokenized and word count is calculated.\n",
    "Args:\n",
    "    pdf_path (str): The path to the PDF file.\n",
    "Returns:\n",
    "    list: A list of dictionaries, each containing information about a page in the PDF, including:\n",
    "        - file_name (str): The name of the PDF file.\n",
    "        - page_number (int): The page number.\n",
    "        - page_word_count (int): The word count of the page.\n",
    "        - page_token_cont (int): The token count of the page.\n",
    "        - text (str): The extracted text from the page.\n",
    "\"\"\"\n",
    "pass\n",
    "\"\"\"\n",
    "Filters out pages with zero word count from the given list of pages and texts.\n",
    "Args:\n",
    "    pages_and_texts_sublist (list): A list of dictionaries, each containing information about a page.\n",
    "Returns:\n",
    "    list: A filtered list of dictionaries, each containing information about a page with a word count greater than zero.\n",
    "\"\"\"\n",
    "pass\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "    for page_number, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "\n",
    "        tokens = tokenizer.encode(text)\n",
    "        word_count = len(text.split())\n",
    "\n",
    "        pages_and_texts.append({\n",
    "            \"file_name\": pdf_path,\n",
    "            \"page_number\": page_number,\n",
    "            \"page_word_count\": word_count,\n",
    "            \"page_token_cont\": len(tokens),\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts =[open_and_read_pdf(local_filename) for local_filename in pdf_files]\n",
    "\n",
    "def get_pages_and_texts(pages_and_texts_sublist):\n",
    "    pages_and_texts = [page for page in pages_and_texts_sublist if page[\"page_word_count\"] > 0]\n",
    "\n",
    "    return pages_and_texts\n",
    "\n",
    "filtered_pages_and_texts =[get_pages_and_texts(pages_and_texts_sublist) for pages_and_texts_sublist in pages_and_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solapar las páginas, de tal manera que una página tenga un trozo de la anterior y de la siguiente, para evitar cortes de bloques. Un 20% es un valor de referencia común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create contextual texts for each PDF by combining text from adjacent pages with a specified overlap ratio.\n",
    "Args:\n",
    "    filtered_pages_and_texts (list of list of dict): A list where each element is a list of dictionaries containing \n",
    "                                                        'text' and 'file_name' keys for each page of a PDF.\n",
    "    overlap_ratio (float, optional): The ratio of text overlap between adjacent pages. Default is 0.2.\n",
    "Returns:\n",
    "    list of dict: A list of dictionaries where each dictionary contains:\n",
    "                    - 'file_name': The name of the PDF file.\n",
    "                    - 'texts': A single concatenated string of contextual texts for the entire PDF.\n",
    "\"\"\"\n",
    "def create_contextual_texts_per_pdf(filtered_pages_and_texts, overlap_ratio=0.2):\n",
    "    all_output_texts = []\n",
    "\n",
    "    for pdf_texts in filtered_pages_and_texts:\n",
    "        output_texts = []\n",
    "        input_texts = [page['text'] for page in pdf_texts]\n",
    "        file_name = pdf_texts[0]['file_name'] if pdf_texts else \"\"\n",
    "\n",
    "        for i in range(len(input_texts)):\n",
    "            previous_context = input_texts[i - 1][-int(len(input_texts[i - 1]) * overlap_ratio/2):] if i > 0 else \"\"\n",
    "            next_context = input_texts[i + 1][:int(len(input_texts[i + 1]) * overlap_ratio/2)] if i < len(input_texts) - 1 else \"\"\n",
    "\n",
    "            combined_text = f\"{previous_context} {input_texts[i]} {next_context}\".strip()\n",
    "\n",
    "            output_texts.append(combined_text)\n",
    "\n",
    "        all_output_texts.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"texts\": output_texts\n",
    "        })\n",
    "\n",
    "    return all_output_texts\n",
    "\n",
    "overlap_ratio = 0.2\n",
    "\n",
    "overlapped_texts_per_pdf = create_contextual_texts_per_pdf(filtered_pages_and_texts, overlap_ratio=overlap_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenamos páginas sucesivas con pocos tokens para tener textos de longitud similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenizes and calculates the token size for each text in the provided list of texts.\n",
    "This script uses the `tiktoken` library to encode texts for the \"gpt-3.5-turbo\" model.\n",
    "It processes a list of dictionaries containing file names and their associated texts,\n",
    "and generates a new list of dictionaries with the file name, text, and the token size.\n",
    "Variables:\n",
    "    tokenizer (tiktoken.Tokenizer): The tokenizer for the \"gpt-3.5-turbo\" model.\n",
    "    input_texts_and_tokens (list): A list of dictionaries, each containing:\n",
    "        - 'file_name' (str): The name of the file.\n",
    "        - 'text' (str): The text content.\n",
    "        - 'token_size' (int): The number of tokens in the text.\n",
    "List comprehensions:\n",
    "    input_texts_and_tokens: Iterates over each text in `overlapped_texts_per_pdf` and\n",
    "    each `texto` in `text['texts']`, encoding the text and calculating its token size.\n",
    "\"\"\"\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "input_texts_and_tokens = [{'file_name': text['file_name'],'text': texto, 'token_size': len(tokenizer.encode(texto))} for text in overlapped_texts_per_pdf for texto in text['texts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concatenates text documents into groups based on a maximum token size.\n",
    "Args:\n",
    "    docs (list of dict): A list of dictionaries where each dictionary represents a document with keys:\n",
    "        - \"file_name\" (str): The name of the file the document belongs to.\n",
    "        - \"text\" (str): The text content of the document.\n",
    "        - \"token_size\" (int): The token size of the document.\n",
    "    max_tokens (int): The maximum number of tokens allowed in a concatenated group.\n",
    "Returns:\n",
    "    list of dict: A list of dictionaries where each dictionary represents a concatenated group of documents with keys:\n",
    "        - \"file_name\" (str): The name of the file the group belongs to.\n",
    "        - \"text\" (str): The concatenated text content of the group.\n",
    "        - \"token_size\" (int): The total token size of the concatenated group.\n",
    "\"\"\"\n",
    "max_tokens = 1000\n",
    "\n",
    "def concatenate_documents(docs, max_tokens):\n",
    "    concatenated_docs = []\n",
    "    current_group = {\"file_name\": \"\", \"text\": \"\", \"token_size\": 0}\n",
    "\n",
    "    for doc in docs:\n",
    "        if current_group[\"file_name\"] != doc[\"file_name\"]:\n",
    "            if current_group[\"token_size\"] > 0:\n",
    "                concatenated_docs.append(current_group)\n",
    "            current_group = {\"file_name\": doc[\"file_name\"], \"text\": doc[\"text\"], \"token_size\": doc[\"token_size\"]}\n",
    "        elif current_group[\"token_size\"] + doc[\"token_size\"] <= max_tokens:\n",
    "            current_group[\"text\"] += (\" \" + doc[\"text\"]).strip()\n",
    "            current_group[\"token_size\"] += doc[\"token_size\"]\n",
    "        else:\n",
    "            concatenated_docs.append(current_group)\n",
    "            current_group = {\"file_name\": doc[\"file_name\"], \"text\": doc[\"text\"], \"token_size\": doc[\"token_size\"]}\n",
    "\n",
    "    if current_group[\"token_size\"] > 0:\n",
    "        concatenated_docs.append(current_group)\n",
    "    \n",
    "    return concatenated_docs\n",
    "\n",
    "concatenated_input_texts_and_tokens = concatenate_documents(input_texts_and_tokens, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script calculates embeddings for a list of text blocks using Azure OpenAI and saves the results to a JSON file.\n",
    "Functions:\n",
    "    calculate_embeddings(text, client=azure_openai_client_embeding):\n",
    "        Calculates embeddings for the given text using the specified Azure OpenAI client.\n",
    "        Args:\n",
    "            text (str): The input text to calculate embeddings for.\n",
    "            client: The Azure OpenAI client to use for generating embeddings.\n",
    "        Returns:\n",
    "            list: The embeddings for the input text.\n",
    "Variables:\n",
    "    text_and_embeddings (list): A list of dictionaries containing block IDs, text, and their corresponding embeddings.\n",
    "    oputput_file (str): The name of the output file where embeddings will be saved.\n",
    "File Operations:\n",
    "    Opens a file named \"Embeddings.json\" in write mode and saves the text and embeddings data in JSON format.\n",
    "    Prints a message indicating the file where embeddings are saved.\n",
    "\"\"\"\n",
    "def calculate_embeddings(text, client=azure_openai_client_embeding):\n",
    "    embeddings = client.embeddings.create(input=text, model=azure_openai_embeding_deployment_name)\n",
    "\n",
    "    return embeddings.data[0].embedding\n",
    "\n",
    "text_and_embeddings = [{'block_id': block_id, 'text': text['text'], 'embeddings': calculate_embeddings(text['text'])} for block_id, text in enumerate(concatenated_input_texts_and_tokens)]\n",
    "\n",
    "oputput_file = \"Embeddings.json\"\n",
    "\n",
    "with open(oputput_file, \"w\") as file:\n",
    "    json.dump(text_and_embeddings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools de búsqueda en índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the cosine similarity between two vectors.\n",
    "Parameters:\n",
    "vec1 (numpy.ndarray): The first vector.\n",
    "vec2 (numpy.ndarray): The second vector.\n",
    "Returns:\n",
    "float: The cosine similarity between vec1 and vec2.\n",
    "\"\"\"\n",
    "# function implementation\n",
    "\"\"\"\n",
    "Find the most similar documents to the input text based on cosine similarity.\n",
    "Parameters:\n",
    "input_text (str): The input text to compare.\n",
    "data (list of dict): A list of dictionaries, each containing 'text' and 'embeddings' keys.\n",
    "desired_doc_count (int, optional): The number of most similar documents to return. Defaults to 1.\n",
    "Returns:\n",
    "list of tuple: A list of tuples, each containing the text and its similarity score, sorted by similarity in descending order.\n",
    "\"\"\"\n",
    "# function implementation\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def find_most_similar(input_text, data, desired_doc_count=0):\n",
    "    input_text_embeding = calculate_embeddings(input_text)\n",
    "    similarities = []\n",
    "    \n",
    "    for entry in data:\n",
    "        similarity = cosine_similarity(input_text_embeding, np.array(entry[\"embeddings\"]))\n",
    "        similarities.append((entry[\"text\"], similarity))\n",
    "\n",
    "    sorted_documents = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Si no hay resultados, devolver None\n",
    "    return sorted_documents[:desired_doc_count] if sorted_documents else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools de búsqueda de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script loads a list of embeddings from a JSON file and provides a function to search for the most similar text based on input text.\n",
    "Functions:\n",
    "    search_for_info(input_text, comparision_data=embedings_list, desired_doc_count=1):\n",
    "        Searches for the most similar text to the input_text within the comparision_data.\n",
    "Parameters:\n",
    "    input_text (str): The text to search for similar entries.\n",
    "    comparision_data (list, optional): The list of embeddings to compare against. Defaults to embedings_list.\n",
    "    desired_doc_count (int, optional): The number of most similar documents to return. Defaults to 1.\n",
    "Returns:\n",
    "    str: The concatenated text of the most similar entries.\n",
    "\"\"\"\n",
    "embedings_list = json.load(open(oputput_file,'r'))\n",
    "\n",
    "embedings_list = [{\n",
    "    \"text\": entry[\"text\"],\n",
    "    \"embeddings\": np.array(entry[\"embeddings\"])\n",
    "} for entry in embedings_list]\n",
    "\n",
    "def search_for_info(input_text,comparision_data=embedings_list,desired_doc_count=1):\n",
    "    most_similar = find_most_similar(input_text, comparision_data, desired_doc_count)\n",
    "    if not most_similar:  # Si la lista está vacía\n",
    "        return None\n",
    "    most_similar_text = \" \".join([entry[0] for entry in most_similar])\n",
    "    return most_similar_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probamos las tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains functions to search for information and data.\n",
    "\n",
    "Functions:\n",
    "    search_for_info(input_text: str) -> str:\n",
    "        Searches for information based on the input text and returns the result.\n",
    "        Example usage:\n",
    "\n",
    "    search_for_data_in_bing(query: str) -> None:\n",
    "        Searches for data on Bing based on the query and prints the result.\n",
    "        Example usage:\n",
    "\"\"\"\n",
    "print(search_for_info(input_text=\"¿Valor nutricional de un plátano?\"))\n",
    "print(search_for_info(input_text=\"¿Tipos de variables en Python?\"))\n",
    "search_for_data_in_bing(\"¿Quién ganó la eurocopa en 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registro de tools para llamada a AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Registers a custom search tool with Azure OpenAI and returns an agent.\n",
    "The custom search tool first searches for information using embeddings and, if no relevant information is found, \n",
    "it falls back to searching for data in Bing. The relevance of the information is determined by calculating the \n",
    "cosine similarity between the query and the response.\n",
    "Returns:\n",
    "    AgentRunner: An agent runner instance with the registered custom search tool.\n",
    "Functions:\n",
    "    custom_agent_worker(query: str) -> str:\n",
    "        Executes the search tools in order and stops the search if relevant information is found.\n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "        Returns:\n",
    "            str: The search result, either from embeddings or Bing.\n",
    "    search_custom:\n",
    "        A FunctionTool instance created from the custom_agent_worker function.\n",
    "    agent_worker:\n",
    "        A FunctionCallingAgentWorker instance created from the search_custom tool and Azure OpenAI client.\n",
    "    agent:\n",
    "        An AgentRunner instance created from the agent_worker.\n",
    "\"\"\"\n",
    "def register_search_tools_with_azure_openai():\n",
    "    def custom_agent_worker(query: str):\n",
    "        \"\"\"Ejecuta las herramientas en orden y detiene la búsqueda si encuentra información.\"\"\"\n",
    "        response = search_for_info(query) \n",
    "        # Verifica si la respuesta es válida (no None, no cadena vacía)\n",
    "        if response and isinstance(response, str) and response.strip():\n",
    "            # Compara la similitud entre la query y la respuesta encontrada\n",
    "            similarity_score = cosine_similarity(calculate_embeddings(query), calculate_embeddings(response))\n",
    "\n",
    "            # Si la similitud es baja (por ejemplo, < 0.4), se considera irrelevante y se devuelve None\n",
    "            print(similarity_score)\n",
    "            if similarity_score >= 0.4:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Respuesta irrelevante, similitud: {similarity_score}. Devolviendo None.\")\n",
    "                return search_for_data_in_bing(query)\n",
    "\n",
    "        return search_for_data_in_bing(query)\n",
    "    \n",
    "    # Convertimos la función en una herramienta compatible con el agente\n",
    "    search_custom = FunctionTool.from_defaults(\n",
    "        fn=custom_agent_worker, \n",
    "        description=\"Busca información en embeddings y, si no encuentra, en Bing.\"\n",
    "    )\n",
    "\n",
    "    agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "        [search_custom],  # Solo se registra la herramienta que coordina ambas búsquedas\n",
    "        llm=azure_openai_client,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    agent = AgentRunner(agent_worker)\n",
    "    return agent\n",
    "\n",
    "# Registrar el agente\n",
    "agent = register_search_tools_with_azure_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamada a API con tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script queries an agent to find out who won the Euro Cup in 2024 and prints the response.\n",
    "Functions:\n",
    "    agent.query(question: str) -> str: Sends a query to the agent and returns the response.\n",
    "Variables:\n",
    "    response (str): The response from the agent to the query \"Quién ganó la eurocopa en 2024?\".\n",
    "Usage:\n",
    "    The script sends a query to the agent asking who won the Euro Cup in 2024 and prints the response.\n",
    "\"\"\"\n",
    "response = agent.query(\"Quién ganó la eurocopa en 2024?\")\n",
    "\n",
    "print(f\"La respuesta es: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
